{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suited-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Identity\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import top_k_top_p_filtering\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, PegasusPreTrainedModel, PegasusModel, PegasusConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sticky-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PegasusWithValueHeadModel(PegasusPreTrainedModel):\n",
    "    \"\"\"The PegasusWithValueHeadModel class implements a Pegasus language model with a secondary, scalar head.\"\"\"\n",
    "    \n",
    "    _keys_to_ignore_on_load_missing = [\n",
    "        r\"final_logits_bias\",\n",
    "        r\"encoder\\.version\",\n",
    "        r\"decoder\\.version\",\n",
    "        r\"lm_head\\.weight\",\n",
    "        r\"embed_positions\\.weight\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, config: PegasusConfig):\n",
    "        super().__init__(config)\n",
    "        self.model = PegasusModel(config)\n",
    "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
    "        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
    "\n",
    "        config.num_labels = 1\n",
    "        self.v_head = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.detach_head = False\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def detach_value_head(self):\n",
    "        self.v_head.detach_head = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "#         token_type_ids=None,\n",
    "#         position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        mc_token_ids=None,\n",
    "        lm_labels=None,\n",
    "        mc_labels=None,\n",
    "        decoder_input_ids=None,\n",
    "    ):\n",
    "       \n",
    "        model_outputs = self.model(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "#             token_type_ids=token_type_ids,\n",
    "#             position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "        \n",
    "        hidden_states = model_outputs.last_hidden_state\n",
    "        \n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        \n",
    "        if self.detach_head:\n",
    "            value = self.v_head(hidden_states.detach()).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            value = self.v_head(hidden_states).squeeze(-1)\n",
    "\n",
    "        outputs = lm_logits, model_outputs[1:], value\n",
    "        return outputs\n",
    "    \n",
    "def respond_to_batch(model, queries, txt_len=20, top_k=0, top_p=1.0):\n",
    "    \"\"\"Sample text from language model.\"\"\"\n",
    "    for i in range(txt_len):\n",
    "        # Get Logits\n",
    "        output = model(**queries)\n",
    "        next_token_logits = output[0][:, -1, :]\n",
    "        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "        # Sample\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "        \n",
    "        # No exploration (No sampling)\n",
    "        # next_token = next_token_logits.argmax(-1)\n",
    "        \n",
    "        queries['decoder_input_ids'] = torch.cat([queries['decoder_input_ids'],\n",
    "                                                      next_token.unsqueeze(-1)], \n",
    "                                                     dim=-1)\n",
    "    return queries['decoder_input_ids'][:, -txt_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "capable-tunnel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusWithValueHeadModel were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['v_head.weight', 'v_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = PegasusWithValueHeadModel.from_pretrained(model_name).to(torch_device)\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_response(input_text,num_return_sequences,num_beams):\n",
    "#     batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "#     translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_returnb_sequences=num_return_sequences, temperature=1.5)\n",
    "#     tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "#     return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "viral-halloween",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  202,  2926, 23682,   196, 14726,   308,   109,  2924,   107,     1,\n",
       "             0,     0],\n",
       "        [  487,   127,   181,  1498,  1784,   112,  5186,   128,  5597,   110,\n",
       "           107,     1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'decoder_input_ids': tensor([[0],\n",
       "        [0]], device='cuda:0')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = ['A massive glacier had crashed down the mountain.', \n",
    "'below are some useful links to facilitate your involvement .']\n",
    "\n",
    "batch_input = tokenizer(input_text ,truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "batch_input['decoder_input_ids'] = torch.zeros(batch_input[\"input_ids\"].shape[0], 1, dtype=int).to(torch_device)\n",
    "batch_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "partial-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode('big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "differential-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(**batch_input)\n",
    "logits, transformer_outputs, values = model(**batch_input)\n",
    "# out = model.model(output_hidden_states=True, **batch_input)\n",
    "# out.last_hidden_state.shape\n",
    "# model.lm_head(out.last_hidden_state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "central-chapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 96103]), torch.Size([2, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tribal-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.decoder_hidden_states[-1].shape, output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "liked-civilization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 202],\n",
       "        [5870]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.argmax(-1)\n",
    "# output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "stupid-mortgage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'A', 'A']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode(202)\n",
    "# [tokenizer.decode(i) for i in output.logits.argmax(-1)]\n",
    "tokenizer.batch_decode(logits.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bright-spray",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch_input['decoder_input_ids'], skip_special_tokens=False)\n",
    "\n",
    "# for j in range(batch_input['input_ids'].shape[0]):\n",
    "#     print([tokenizer.decode(i) for i in batch_input['decoder_input_ids'][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "continuous-investigator",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20])\n",
      "['A', 'glacier', 'crashed', 'down', 'the', 'mountain', '.', '</s>', '</s>', 'Merrick', 'persevered', 'disastrous', 'ly', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "['links', 'that', 'are', 'helpful', 'can', 'be', 'found', 'below', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', 'chette', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "resp = respond_to_batch(model, dict(batch_input))\n",
    "print(resp.shape)\n",
    "\n",
    "# tokenizer.batch_decode(resp, skip_special_tokens=False)\n",
    "for j in range(resp.shape[0]):\n",
    "    print([tokenizer.decode(i) for i in resp[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "acute-arrest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A glacier crashed down the mountain.'"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(resp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "legislative-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_to_batch(model, queries, txt_len=20, top_k=0, top_p=1.0):\n",
    "    \"\"\"Sample text from language model.\"\"\"\n",
    "    for i in range(txt_len):\n",
    "        # Get Logits\n",
    "        output = model(**queries)\n",
    "        next_token_logits = output[0][:, -1, :]\n",
    "        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "        # Sample\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "        \n",
    "        # No exploration (No sampling)\n",
    "#         next_token = next_token_logits.argmax(-1)\n",
    "        \n",
    "        \n",
    "        queries['decoder_input_ids'] = torch.cat([queries['decoder_input_ids'],\n",
    "                                                      next_token.unsqueeze(-1)], \n",
    "                                                     dim=-1)\n",
    "    return queries['decoder_input_ids'][:, -txt_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "spectacular-dress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', 'The', 'glacier', 'crashed', 'down', 'the', 'mountain', '.', '</s>']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(i) for i in model.generate(**batch_input)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "found-mandate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   139, 23682, 14726,   308,   109,  2924,   107,     1]],\n",
       "       device='cuda:2')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(**batch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "numerical-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = torch.zeros(4,5)\n",
    "zz[:, -1] += torch.ones(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "ranging-ocean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-boston",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-proj-simp",
   "language": "python",
   "name": "rl-proj-simp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
